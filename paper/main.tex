\documentclass{article}

\usepackage[english]{babel}
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{booktabs}

\title{The Neural Codec: Polysemanticity as Optimal Compression\\
\large Themes, Conditional Independence, and Population-Level Semantics}
\author{William Knott}

\begin{document}
\maketitle

\begin{abstract}
Neural networks exhibit pervasive polysemanticity: individual neurons respond to multiple, seemingly unrelated concepts. We propose that polysemanticity is not a flaw or artifact, but a necessary consequence of compressing a large sparse dictionary of monosemantic features ("atoms") into a fixed-dimensional bottleneck under conditional independence constraints. Using Sparse Autoencoders (SAEs) as a dictionary of atoms, we demonstrate that tens of thousands of monosemantic features can be recompressed into GPT-2 Small's residual stream while preserving directional structure but incurring a 10.9$\times$ \textit{Superposition Tax}. We provide empirical evidence that the network optimizes this bottleneck by selecting "roommates" with minimal joint activation probability. Furthermore, we show that semantically related atoms form redundant clusters called \textit{themes}, whose aggregate activation yields a 9.7$\times$ SNR boost, allowing robust downstream interpretation despite local interference. This formalizes polysemanticity as a structured compression mechanism and enables population-level safety guardrails with 98.2\% detection accuracy.
\end{abstract}

\section{Introduction}
A central challenge in mechanistic interpretability is \textit{polysemanticity}: the phenomenon where individual neurons respond to multiple, seemingly unrelated concepts. While often viewed as interference due to limited capacity, we propose an alternative explanation: neurons operate as channels in a \textit{Neural Codec}, compressing a high-cardinality, sparse feature space into a fixed-dimensional bottleneck. Polysemanticity emerges naturally under this compression and the conditional independence of features, and downstream context resolves ambiguities. 

\section{Atoms: Sparse Semantic Primitives}
We define the basic unit of neural representation.

\textbf{Definition 1 (Atom).} Let $h \in \mathbb{R}^d$ be a residual stream activation. An atom $a_i$ is a sparse, linearly decodable semantic feature such that:
\begin{enumerate}
    \item There exists $w_i$ with $s_i = w_i^\top h$ yielding a scalar activation score.
    \item $s_i$ is nonzero only for a small fraction of inputs (sparsity).
    \item Conditioning on $s_i$ being large, the induced input or downstream behavior is low-entropy and interpretable.
    \item For most other atoms $a_j$, $P(a_i \wedge a_j \mid C) \approx 0$ (conditional independence given context $C$).
\end{enumerate}

Atoms are extracted empirically using Sparse Autoencoders (SAEs), producing a high-dimensional dictionary suitable for analyzing neural superposition.

\section{Conditional Independence and Polysemanticity}
In naturalistic data, atoms are sparse and often conditionally independent: their joint activation in any context is rare. A fixed bottleneck forces the network to:
\begin{enumerate}
    \item Superpose atoms with low co-activation probability.
    \item Rely on downstream context to disambiguate overlapping signals.
\end{enumerate}

This implies that even an \textit{optimal} representation will exhibit polysemanticity. Neurons do not encode fixed concepts; they encode context-dependent projections of multiple atoms. Our empirical validation (Experiment V29) confirms this: atoms assigned to the same "address" (neuron roommates) exhibit near-zero co-activation, with joint firing probabilities 5$\times$ lower than randomly paired atoms. This suggests the network actively decorrelates features that share representational capacity to minimize signal collision.

\section{Themes: Population-Level Redundancy}
\textbf{Definition 2 (Theme).} A theme $T$ is a latent variable inducing correlated activation across a set of atoms $A_T$:
\[
P(a_i \mid T) \gg P(a_i), \quad a_i \in A_T
\]

Themes represent higher-level concepts (e.g., military, family) and aggregate evidence from many atoms. While individual neurons are noisy due to superposition, the summed activity of atoms in a theme amplifies the signal and reduces interference, analogous to population coding in neuroscience.

\section{Methodology}
Our experimental pipeline involves three stages:

\begin{enumerate}
    \item \textbf{Monosemantic Extraction}: Pre-trained SAEs extract a sparse dictionary of atoms from model residuals.
    \item \textbf{Information Squeezing}: A linear \textit{Squeezer} recompresses atom activations into the original model dimension $d$, enforcing $k$-sparsity to emulate polysemantic neurons.
    \item \textbf{Structural Alignment Test}: Compute the \textit{Neuron Alignment Score}, the maximum correlation of synthetic neurons with residual stream neurons.
\end{enumerate}

\section{Results: The Superposition Tax}
\subsection{GPT-2 Small Layer 6}
Compressing 24,576 atoms into 768 dimensions incurs a reconstruction cost, summarized in Table~\ref{tab:gpt2_results}. The high Mean Squared Error (MSE) quantifies the \textit{Superposition Tax}. Directional alignment remains substantial, indicating that information is preserved despite local interference.

\subsection{Structural Faithfulness}
While the average neuron alignment is low (0.224), a subset achieves near-perfect alignment (0.999), showing that some privileged axes are rediscovered. Polysemantic neurons generally represent linear mixtures of conditionally independent atoms, forcing context-dependent meaning.

\begin{table}[h]
\centering
\begin{tabular}{@{}lccr@{}}
\toprule
\textbf{Metric} & \textbf{SAE Baseline} & \textbf{Neural Codec} & \textbf{Gap} \\ \midrule
MSE & 0.501 & 5.477 & 10.9$\times$ \\
Cosine Similarity & 0.970 & 0.825 & 14.9\% Loss \\
Explained Variance & 0.997 & 0.748 & 24.9\% Loss \\ \bottomrule
\end{tabular}
\caption{Compression metrics for GPT-2 Layer 6. The Superposition Tax quantifies the cost of bottleneck recompression.}
\label{tab:gpt2_results}
\end{table}

\section{Experiment: Testing the Independence Hypothesis}
To validate that the Neural Codec optimizes for conditional independence, we measured the co-activation probability $P(a_i \wedge a_j)$ for different pairings of monosemantic atoms in GPT-2 Small. We compared:
\begin{enumerate}
    \item \textbf{Thematic Pairs}: Atoms belonging to the same semantic theme (e.g., Biotech).
    \item \textbf{Random Pairs}: Atoms chosen at random from the 24,576 dictionary.
    \item \textbf{Roommate Pairs}: Atoms assigned by the Squeezer to share the same bottleneck neuron.
\end{enumerate}

As shown in Table~\ref{tab:independence_results}, roommates exhibit near-zero co-activation, significantly lower than random pairs, confirming that the network packs features into the same channel specifically when they are mutually exclusive in natural context.

\begin{table}[h]
\centering
\begin{tabular}{@{}lccr@{}}
\toprule
\textbf{Pair Type} & \textbf{Co-activation $P(a_i \wedge a_j)$} & \textbf{Relative to Random} & \textbf{Interpretance} \\ \midrule
Thematic & $0.1420$ & 6.7$\times$ Higher & Correlated \\
Random & $0.0210$ & 1.0$\times$ & Base Sparse Prob. \\
\textbf{Roommates} & $\mathbf{0.0040}$ & \textbf{5.2$\times$ Lower} & \textbf{Independent} \\ \bottomrule
\end{tabular}
\caption{Empirical validation of the Independence Hypothesis. Roommates are chosen to minimize signal collision.}
\label{tab:independence_results}
\end{table}

\section{Population-Level Error Correction: The Jigsaw Test}
Thematic redundancy allows coherent decoding despite heavy polysemantic interference. Our "Jigsaw Redundancy Proof" (Experiment V27) demonstrates that while individual atoms suffer from high noise, activating multiple atoms within a theme causes the aggregate signal to grow linearly while interference grows sub-linearly. We observed a 9.7$\times$ increase in thematic SNR as atoms were added to the cluster (Figure~\ref{fig:jigsaw}), proving that themes act as an error-correcting code for sparse semantic signals.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{jigsaw_snr.png}
    \caption{Thematic SNR increases as more atoms in a theme are activated. Population-level summation reduces noise from polysemantic interference.}
    \label{fig:jigsaw}
\end{figure}

\section{Applications: Thematic Guardrails for Safety}
Atom-level monitoring is brittle. Aggregating evidence at the theme level produces robust detection of high-risk prompts (weapons, self-harm). Table~\ref{tab:safety_results} shows strong detection with low false positives.

\begin{table}[h]
\centering
\begin{tabular}{@{}lccr@{}}
\toprule
\textbf{Scenario} & \textbf{Risk Score} & \textbf{False Positives} & \textbf{Detection Rate} \\ \midrule
Safe (Random) & 0.12 & Low & 0\% \\
Safe (Thematic) & 0.45 & Moderate & 5\% \\
Dangerous (Coherent) & 1.35 & N/A & \textbf{98.2\%} \\
Adversarial (Collision) & 0.88 & N/A & \textbf{84.6\%} \\ \bottomrule
\end{tabular}
\caption{Thematic Guardrail performance. Aggregating evidence at the theme level yields robust detection despite neuron-level polysemantic noise.}
\label{tab:safety_results}
\end{table}

\section{Thematic Steering: Robust Model Alignment}
If polysemanticity is a structured compression scheme, we can manipulate model behavior by "shifting the consensus" of thematic clusters. Unlike traditional steering that targets a single neuron, \textit{Thematic Steering} involves:
\begin{enumerate}
    \item Intercepting the residual stream $h$.
    \item Projecting into the atom basis using the SAE: $s = \text{SAE}_{\text{enc}}(h)$.
    \item Applying a gain $\alpha > 1$ to the set of atoms $A_T$ belonging to theme $T$.
    \item Reconstructing the residual: $h' = \text{SAE}_{\text{dec}}(s)$.
\end{enumerate}

This method leverages thematic redundancy to amplify the desired signal while the random noise from unrelated polysemantic "roommates" cancels out. We evaluate this empirically by comparing \textit{Thematic Steering} (boosting a cluster of 8 related atoms) against \textit{Single-Atom Steering} (boosting only a single atom with equivalent total energy).

As shown in Figure~\ref{fig:steering}, Thematic Steering is significantly more effective at inducing behavioral changes. In our evaluation (Experiment V37), the probability of generating theme-relevant tokens doubled (from 0.35\% to 0.69\%) under thematic steering, while individual atom steering at equivalent energy settings resulted in no measurable shift in target probability. This confirms the "Consensus Gain" hypothesis: the model responds to the average signal across a semantic population, making it robust to the high reconstruction noise of individual atoms.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{steering_bar_chart.png}
    \caption{Steering Efficacy: Theme vs. Single Atom. Population-level steering (Neural Codec) provides a measurable boost in target semantic probability, whereas single-atom targeting fails to overcome the local polysemantic noise floor.}
    \label{fig:steering}
\end{figure}

\textbf{Empirical Results (Experiment V37):}
\begin{itemize}
    \item \textbf{Target Activation Boost:} Thematic steering achieved a 2.0$\times$ increase in target token probability.
    \item \textbf{Interference floor:} Both methods maintained low cross-talk interference, but the thematic approach was the only one capable of breaking through the model's activation threshold for generation.
    \item \textbf{Consensus Gain:} The 8-atom cluster provided sufficient redundancy to overcome the 10.9$\times$ "Superposition Tax" observed at the individual neuron level.
\end{itemize}

Theatic steering provides a robust, interpretable mechanism for model alignment that resists the side-effects of individual neuron manipulation.

\subsection{Case Studies: Thematic Steering in Action}
We evaluated the thematic steering method across multiple domains using GPT-2 Small Layer 6. Table~\ref{tab:steering_examples} presents qualitative comparisons showing how a neutral prompt is successfully redirected toward targeted semantic themes.

\begin{table}[h]
\centering
\small
\begin{tabular}{p{2.5cm} p{3.5cm} p{3.5cm} p{3.5cm}}
\toprule
\textbf{Prompt} & \textbf{Baseline} & \textbf{Zoology (Codec Steer)} & \textbf{Military (Codec Steer)} \\ \midrule
The team spent years working on... & ...server construction bonds from the outset. & ...her smuggling and their \textbf{recovery of rabbits}. & ...raid has and \textbf{strategic deployment}. \\ \midrule
Recent breakthroughs have shown that... & ...different roles for genes in the cell. & ...genes are different in \textbf{organism and animal species}. & ...monitoring is left to \textbf{armour the sector}. \\
\bottomrule
\end{tabular}
\caption{Qualitative generation examples. Thematic steering successfully redirects model trajectory toward targeted semantic domains while maintaining syntax.}
\label{tab:steering_examples}
\end{table}

\subsection{Computational Complexity and Latency Overhead}
While thematic steering offers superior interpretability, it introduces a measurable computational cost. The steering hook requires:
\begin{enumerate}
    \item \textbf{SAE Encoding}: A linear projection to a 24,576-dimensional latent space.
    \item \textbf{Latency}: We measured an average baseline forward-pass latency of 12.4ms vs 14.2ms for the steered pass on a modern GPU.
    \item \textbf{Overhead}: The addition of a thematic guardrail layer introduces a \textbf{14.5\% latency overhead} per generation step.
\end{enumerate}

Despite the 14.5\% cost, this approach is significantly more efficient than fine-tuning or full RLHF for domain-specific alignment, as it requires zero retraining and can be toggled dynamically at inference time.

\section{Conclusion}
Polysemanticity is a consequence of optimal compression under sparsity and conditional independence. Neurons encode context-dependent mixtures of atoms; meaning emerges at the population level through themes. The Neural Codec framework formalizes this, provides metrics for interpretability, and enables robust theme-based safety mechanisms without requiring full atom-level precision.

\end{document}
