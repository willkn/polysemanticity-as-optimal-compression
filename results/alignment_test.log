`torch_dtype` is deprecated! Use `dtype` instead!
/opt/miniconda3/lib/python3.13/site-packages/sae_lens/saes/sae.py:248: UserWarning: 
This SAE has non-empty model_from_pretrained_kwargs. 
For optimal performance, load the model like so:
model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)
  warnings.warn(
/Users/kevknott/will/research/ARA/projects/the-neural-codec/experiments/gpt2_neuron_alignment.py:28: DeprecationWarning: Unpacking SAE objects is deprecated. SAE.from_pretrained() now returns only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() to get the config dict and sparsity as well.
  sae, _, _ = SAE.from_pretrained(release=SAE_RELEASE, sae_id=SAE_ID, device=DEVICE)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
üöÄ Loading gpt2-small and SAE on mps...
Loaded pretrained model gpt2-small into HookedTransformer
üèãÔ∏è Training with Neuron Alignment Monitoring...
Training:   0%|          | 0/300 [00:00<?, ?it/s]Training:   0%|          | 1/300 [00:48<4:03:16, 48.82s/it]Training:   1%|          | 2/300 [00:49<1:41:32, 20.44s/it]Training:   1%|          | 3/300 [00:50<58:08, 11.75s/it]  Training:   1%|‚ñè         | 4/300 [00:52<38:45,  7.86s/it]Training:   2%|‚ñè         | 5/300 [00:56<31:04,  6.32s/it]Training:   2%|‚ñè         | 6/300 [00:59<25:38,  5.23s/it]Training:   2%|‚ñè         | 7/300 [01:02<22:44,  4.66s/it]Training:   3%|‚ñé         | 8/300 [01:07<22:01,  4.52s/it]Training:   3%|‚ñé         | 9/300 [01:09<18:16,  3.77s/it]Training:   3%|‚ñé         | 10/300 [01:12<18:07,  3.75s/it]Training:   4%|‚ñé         | 11/300 [01:16<17:28,  3.63s/it]Training:   4%|‚ñç         | 12/300 [01:22<21:08,  4.40s/it]Training:   4%|‚ñç         | 13/300 [01:27<21:54,  4.58s/it]Training:   5%|‚ñç         | 14/300 [01:30<20:11,  4.24s/it]Training:   5%|‚ñå         | 15/300 [01:37<23:42,  4.99s/it]Training:   5%|‚ñå         | 16/300 [01:44<26:13,  5.54s/it]Training:   6%|‚ñå         | 17/300 [01:56<35:46,  7.59s/it]Training:   6%|‚ñå         | 18/300 [02:03<34:27,  7.33s/it]Training:   6%|‚ñã         | 19/300 [02:19<46:03,  9.83s/it]